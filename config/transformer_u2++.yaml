data:
  audio:
    filter_conformer_conf:
      output_len_prob: 0.9
    log_mel_conf:
      n_mels: 80
      sample_rate: 16000
      window: hamming
      window_size_sec: 0.025
      window_stride_sec: 0.01
    normalize: true
    pad_token_id: 0
    spec_aug_conf:
      freq_mask_cnt: 2
      freq_mask_para: 10
      time_mask_cnt: 2
      time_mask_para: 50
    spec_sub_conf:
      max_t: 30
      num_t_sub: 3
    spec_trim_conf:
      max_t: 50
    speed_aug_conf:
      speeds: [0.9,1.0,1.1]
  text:
    bos_token_id: 2
    eos_token_id: 3
    pad_token_id: 0
model:
  ctc_reduction: sum
  ctc_weight: 0.3
  ctc_zero_inf: true
  decoder:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 3
    r_num_blocks: 3
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0
  encoder:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d # encoder architecture type
    normalize_before: true
    use_dynamic_chunk: true
    use_dynamic_left_chunk: false
  length_normalized_loss: false
  lsm_weight: 0.1
  reverse_weight: 0.3
  vocab_size: 72
